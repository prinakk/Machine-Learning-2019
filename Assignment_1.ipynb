{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prinakk/Machine-Learning-2019/blob/master/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBL0unt6Wx6-",
        "colab_type": "text"
      },
      "source": [
        "# TITLE\n",
        "**Literature review on \" LONG-SHORT TERM MEMORY \" by Hochreiter and Schmidhuber**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixQM4uRwWyNt",
        "colab_type": "text"
      },
      "source": [
        "# INTRODUCTION\n",
        "\n",
        "To mimic the behaviour of a human mind, researchers and scientists created neural networks. Though there have been instrumental progress in neural networks, traditional neural networks which follows a feedforward system cannot recollect things it has learned. This system doesn't do justice to its original intention of mimicking the human brain, as recalling something is a significant aspect of the human mind. To overcome this obstacle, researchers and scientists came up with recurrent neural networks. Recurrent neural networks have a unique architecture which enables them to retain data for a short period. RNN has been instrumental in time series forecasting,  music composition, natural language processing. With the advent of RNN's in 1986 by David Rumelhart, there has been a rapid progression in the technique, and lots of architectures have been developed. In traditional RNN, data which is learned is turned into gradients and backpropagated to the next data input, and these gradients, however, bring about two major problems. 1. Gradient Vanishing  2. Gradient exploding\n",
        "\n",
        "The paper titled \" LONG-SHORT TERM MEMORY \" by Hochreiter and Schmidhuber deals with tackling the problems present in traditional RNN's and introduce a method to overcome it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VckWftbRW1IJ",
        "colab_type": "text"
      },
      "source": [
        "# CONTENT\n",
        "\n",
        " This paper introduces a novel RNN architecture called Long Short-Term Memory. Traditional RNN's use feedback connections to represent past inputs in the form of activations to future inputs. This act of converting them into activation sequences makes the memory last for a shorter duration of time(Short-Term Memory). A conventional method of RNN uses Back-Propagation Through Time ( Williams and Zipser 1992). This method has diverse applications including speech processing, music control and non-Markovian control. Though being widely used, this method takes too much time to learn what to put in the short-term memory, so for inputs which have minimal time lag between them, they fail to function correctly. As mentioned in the previous section, these gradients or activation sequences which are backpropagated to the future inputs face the risk of vanishing or exploding. The Long Short-Term memory method is a gradient-based learning algorithm which is designed to overcome the back-propagation errors. This algorithm enables long term dependencies which were a drawback of traditional backpropagated RNN's. \n",
        "\n",
        "The paper also outlines previous works and presents a detailed analysis of the problem it seeks to solve, i.e. Conventional BPTT (Back Propagation Through Time). The authors have presented their solution and have performed six experiments and have produced the results of each trial.\n",
        "The paper also outlines the disadvantages of the method proposed along with its advantages over previous algorithms in RNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qSMVYQ2W1zQ",
        "colab_type": "text"
      },
      "source": [
        "# INNOVATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbqgWIadXF2h",
        "colab_type": "text"
      },
      "source": [
        "# TECHNICAL QUALITY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJoIMFghXHrh",
        "colab_type": "text"
      },
      "source": [
        "# APPLICATION AND X-FACTOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa2AIePLSOjP",
        "colab_type": "text"
      },
      "source": [
        "# PRESENTATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkgd-2ahXJpw",
        "colab_type": "text"
      },
      "source": [
        "# REFERENCES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S_lJeNlWwwW",
        "colab_type": "text"
      },
      "source": [
        "# Link to Github\n",
        "\n",
        "[Link to Github Repository](https://github.com/prinakk/Machine-Learning-2019.git)\n",
        "\n"
      ]
    }
  ]
}
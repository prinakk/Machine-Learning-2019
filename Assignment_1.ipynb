{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prinakk/Machine-Learning-2019/blob/master/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBL0unt6Wx6-",
        "colab_type": "text"
      },
      "source": [
        "# TITLE\n",
        "**Literature review on \" LONG-SHORT TERM MEMORY \" by Hochreiter and Schmidhuber 1997**\n",
        "\n",
        "**Name: Prina Kamakotti**\n",
        "\n",
        "**Student ID: 13171780**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixQM4uRwWyNt",
        "colab_type": "text"
      },
      "source": [
        "# INTRODUCTION\n",
        "\n",
        "To mimic the behaviour of a human mind, researchers and scientists created neural networks. Though there have been instrumental progress in neural networks, traditional neural networks which follows a feedforward system cannot recollect things it has learned. This system doesn't do justice to its original intention of mimicking the human brain, as recalling something is a significant aspect of the human mind. To overcome this obstacle, researchers and scientists came up with recurrent neural networks. Recurrent neural networks have a unique architecture which enables them to retain data for a short period. RNN has been instrumental in time series forecasting,  music composition, natural language processing. With the advent of RNN's in 1986 by David Rumelhart, there has been a rapid progression in the technique, and lots of architectures have been developed. In traditional RNN, data which is learned is turned into gradients and backpropagated to the next data input, and these gradients, however, bring about two major problems. 1. Gradient Vanishing  2. Gradient exploding\n",
        "\n",
        "The paper titled \" LONG-SHORT TERM MEMORY \" by Hochreiter and Schmidhuber deals with tackling the problems present in traditional RNN's and introduce a method to overcome it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VckWftbRW1IJ",
        "colab_type": "text"
      },
      "source": [
        "# CONTENT\n",
        "\n",
        " This paper introduces a novel RNN architecture called Long Short-Term Memory. Traditional RNN's use feedback connections to represent past inputs in the form of activations to future inputs. This act of converting them into activation sequences makes the memory last for a shorter duration of time(Short-Term Memory). A conventional method of RNN uses Back-Propagation Through Time (Williams and Zipser 1992). This method has diverse applications including speech processing, music control and non-Markovian control. Though being widely used, this method takes too much time to learn what to put in the short-term memory, so for inputs which have minimal time lag between them, they fail to function correctly. As mentioned in the previous section, these gradients or activation sequences which are backpropagated to the future inputs face the risk of vanishing or exploding. The Long Short-Term memory method is a gradient-based learning algorithm which is designed to overcome the back-propagation errors. This method truncates the gradient to makes sure it does not harm by vanishing or exploding. Long Short-Term Memory (LSTM) can function in instances where the minimal time lag between the inputs is more than 1000 discrete time steps by introducing a constant error flow back to the future inputs through these truncated gradients. This algorithm enables long term dependencies which were a drawback of traditional backpropagated RNN's. \n",
        "\n",
        "The paper also outlines previous works and presents a detailed analysis of the problem it seeks to solve, i.e. Conventional BPTT (Back Propagation Through Time). The authors have given their solution and have performed six experiments with simulated data with multiple variations, and they have addressed the results of each trial.\n",
        "The paper also outlines the disadvantages of the method proposed along with its advantages over previous algorithms in RNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qSMVYQ2W1zQ",
        "colab_type": "text"
      },
      "source": [
        "# INNOVATION\n",
        "LSTM stands out from previous models from the fact that it can solve the long term dependency problems present in conventional RNN's and  that it can solve the two backpropagation problems common in RNN's. LSTM can facilitate this because Hochreiter and Schmidhuber created a new model which uses gates and a Memory cell.\n",
        "\n",
        "Conventional RNN's use the current state of the cell ($C_{t}$), the state of the hidden layer ($H_{t}$) and the input ($X_{t}$), where the current state of the cell ($C_{t}$) depends on the state of the hidden layer of the present ($H_{t}$) and past state of the hidden layer ($H_{t-1}$) , and the input state ($X_{t}$). Whereas in LSTM the current state ($C_{t}$) depends on not only the states mentioned above,  it depends on certain gates introduced by Hochreiter and Schmidhuber; They developed three gates to determine which information the model should retain and which information it has to forget, the two gates are 1. Input Gate 2. Output Gate. The two gates combine to form the memory cell, which is responsible for the current state of the model. The input gate is a multiplicative gate accountable for determining what new information to be added to cell state ($C_{t}$) from current input ($X_{t}$), the forget gate is also a multiplicative gate which is responsible for determining what should be remembered from the previous cell state ($C_{t-1}$).\n",
        "\n",
        "There are connections in and out of the memory cell, the weights of these gates, along with the memory cell, facilitate the long term dependencies in the LSTM model. Through these multiplicative gates, an activation function is required, and the logistic sigmoid function is used.It solves the problem of exploding and vanishing gradients by truncating the gradients through the multiplicative gates, which introduces a constant error flow back to the inputs regardless of the time lag in the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbqgWIadXF2h",
        "colab_type": "text"
      },
      "source": [
        "# TECHNICAL QUALITY\n",
        "The overall technical quality of the paper is good, and the authors have done an excellent job with analysis of past works in the field and giving a comparison to their proposed model. The paper provides a detailed analysis of the const error backdrops present in conventional RNN's. They present a comprehensive review of the exponentially decaying error and the constant error flow present in conventional RNN's. After the analysis of the said errors, they go about presenting a solution in the form of their proposed model and how it tackles the errors in question.\n",
        "\n",
        "The experimentation to prove their model by the authors is very conclusive and extensive. They have performed six different experimentations with simulated data, which can be easily reproduced by anyone reading the paper to verify its authenticity. They perform these experiments on their model, along with previous works and models. They compare the performance of their model to the simulated data with the other models, and the results of the performance have been tabulated for each model. Though the model has been tested with artificial data rather than real-time data, the authors have made an effort to compare their model with others in this training data and show its advantages. The six experiments carried out are in a bid to simulate real-life instances or changes that might be present, i.e. They include noise to the inputs to alter their performance, they verify its performance in adding and multiplication problems. Such detailed experiments can mimic real-time data which might be present in the real world. \n",
        "\n",
        "I find the technical quality of the paper very engaging and find it far ahead of its time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJoIMFghXHrh",
        "colab_type": "text"
      },
      "source": [
        "# APPLICATION AND X-FACTOR\n",
        "The application domain mentioned by the authors are very suitable for the model they have proposed. Currently many natural language processing models use LSTM's as their core. Though developed in 1997 this model remains way ahead of its time in its application to said fields, some of the significant applications of the LSTM models are:\n",
        "\n",
        "1.Natural Language processing\n",
        "\n",
        "2.Music Composition\n",
        "\n",
        "3.Series Prediction\n",
        "\n",
        "4.Handwriting recognition\n",
        "\n",
        "5.Machine Translation\n",
        "\n",
        "6.Sentimental Analysis\n",
        "\n",
        "Hochreiter and Schmidhuber proposed a memory cell with two gates, namely the input and output gate, whereas after further research scholars have come up with four such gates which improve the performance of said LSTM model. The addition of the Forget and input modulation gate have improved the efficacy of the LSTM model. The most exciting aspect I found about this paper was the solidarity and intellect required to come up with a completely new model to radicalize time-series predictions. The X-factor, according to me, is the way they used a completely new idea of using gates and truncating the gradients to solve the common back propagation errors, this step has revolutionized the whole RNN method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa2AIePLSOjP",
        "colab_type": "text"
      },
      "source": [
        "# PRESENTATION\n",
        "In general, the overall structure of the paper is outstanding. The authors start the paper with an introduction to RNN's, the problem they seek to address and the remedy they have presented. The authors have also made an effort to point out the overall structure of their paper by splitting it into different sections and the knowledge flow across these sections.\n",
        "\n",
        "They have given a brief analysis of past models in a section,  they have analysed the problems present in conventional RNN's in one part and then proceeded to explain their novel method to remedy the issues. They then continue to conduct experiments with their model and compare its performance with other known models.\n",
        "The authors finally conclude their research with the limitations of their model along with the advantages. Reading through the paper was very efficient, and I had no issue in what information the authors wanted to put across.\n",
        "\n",
        "This paper tends to be more inclined towards more mathematically inclined scholars, as I do not fit into that niche category, I found it hard to understand the sheer amount of formulae used to explain their research and had to refer other texts and papers to get a general idea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkgd-2ahXJpw",
        "colab_type": "text"
      },
      "source": [
        "# REFERENCES\n",
        "1.Baddeley, A. 2003, 'Working memory and language: An overview', Journal of communication disorders, vol. 36, no. 3, pp. 189-208.\n",
        "\n",
        "\n",
        "2.Bakker, B. 2002, 'Reinforcement learning with long short-term memory', pp. 1475-82.\n",
        "\n",
        "3.Bliss, T.V. & Collingridge, G.L. 1993, 'A synaptic model of memory: long-term potentiation in the hippocampus', Nature, vol. 361, no. 6407, p. 31.\n",
        "\n",
        "\n",
        "4.Graves, A. 2012, 'Long short-term memory', Supervised sequence labelling with recurrent neural networks, Springer, pp. 37-45.\n",
        "\n",
        "\n",
        "5.Hochreiter, S., Bengio, Y., Frasconi, P. & Schmidhuber, J. 2001, 'Gradient flow in recurrent nets: the difficulty of learning long-term dependencies', Series Gradient flow in recurrent nets: the difficulty of learning long-term dependencies A field guide to dynamical recurrent neural networks. IEEE Press.\n",
        "\n",
        "\n",
        "6.Hochreiter, S. & Schmidhuber, J. 1997, 'Long short-term memory', Neural computation, vol. 9, no. 8, pp. 1735-80.\n",
        "\n",
        "\n",
        "7.Russell, E.W. 1975, 'A multiple scoring method for the assessment of complex memory functions', Journal of Consulting and Clinical Psychology, vol. 43, no. 6, p. 800.\n",
        "\n",
        "\n",
        "8.Warrington, E.K. & Weiskrantz, L. 1968, 'New method of testing long-term retention with special reference to amnesic patients', Nature, vol. 217, no. 5132, p. 972.\n",
        "\n",
        "\n",
        "9.Williams, J.M.G., Teasdale, J.D., Segal, Z.V. & Soulsby, J. 2000, 'Mindfulness-based cognitive therapy reduces overgeneral autobiographical memory in formerly depressed patients', Journal of abnormal psychology, vol. 109, no. 1, p. 150.\n",
        "\n",
        "10.Williams, R.J. & Zipser, D. 1995, 'Gradient-based learning algorithms for recurrent', Backpropagation: Theory, architectures, and applications, vol. 433.\n",
        "\n",
        "11.Zhu, X., Sobihani, P. & Guo, H. 2015, 'Long short-term memory over recursive structures', pp. 1604-12."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S_lJeNlWwwW",
        "colab_type": "text"
      },
      "source": [
        "# Link to Github\n",
        "\n",
        "REPO : https://github.com/prinakk/Machine-Learning-2019.git\n",
        "\n",
        "\n",
        "ASSIGNMENT : https://github.com/prinakk/Machine-Learning-2019/blob/master/Assignment_1.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQEJQzoLXOl8",
        "colab_type": "text"
      },
      "source": [
        "# EXPERIMENT AREA"
      ]
    }
  ]
}